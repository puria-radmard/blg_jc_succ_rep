{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to successor representations\n",
    "\n",
    "BLG JC 06.06.2025\n",
    "\n",
    "Daniel Kornai & Puria Radmard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "Assuming a basic awareness of MDPs and RL\n",
    "\n",
    "$i,j\\in\\mathcal{N}$ - non-terminal states\n",
    "\n",
    "$k\\in\\mathcal{T}$ - terminal states\n",
    "\n",
    "$Q_{ij}$ - probabiltiy of moving from state $i$ to non-terminal state $j$\n",
    "\n",
    "$s_{ik}$ - probabiltiy of moving from state $i$ to terminal state $k$\n",
    "\n",
    "$z_k$ - reward from terminating at state $k$, with mean $\\bar{z}_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected returns\n",
    "\n",
    "Vector of *immediate* mean return: $\\boldsymbol{h}: h_i = \\sum_{k\\in\\mathcal{T}} s_{ik} \\bar{z}_k$ - i.e. get reward by moving from $i$ directly to $k$\n",
    "\n",
    "Vector of *overall* mean return with no discount factor: $\\bar{\\boldsymbol{r}} = \\boldsymbol{h} + Q\\boldsymbol{h} + Q^2\\boldsymbol{h} ... = [I - Q]^{-1}\\boldsymbol{h}$\n",
    "\n",
    "Vector of *overall* mean return with discount factor $\\gamma$: $\\bar{\\boldsymbol{r}} = \\boldsymbol{h} + \\gamma Q\\boldsymbol{h} + \\gamma ^2Q^2\\boldsymbol{h} ... = [I - \\gamma Q]^{-1}\\boldsymbol{h}$\n",
    "\n",
    "Discount: $\\bar{\\boldsymbol{r}} = \\boldsymbol{h} + \\gamma Q \\bar{\\boldsymbol{r}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value approximation\n",
    "\n",
    "We represent each state $i$ with vector $\\boldsymbol{x}_i$\n",
    "\n",
    "Want to learn an approximation $\\hat{r}(i;\\boldsymbol{w}) = \\boldsymbol{x}_i^\\intercal \\boldsymbol{w} \\approx \\bar{r}_i$\n",
    "\n",
    "i.e. $\\hat{\\boldsymbol{r}}(\\boldsymbol{w}) = X^\\intercal \\boldsymbol{w}$, where $X$ is the stacked representation vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal difference learning of value approximation\n",
    "\n",
    "In this simple setup where we do not get a reward for steps between non-terminal states - reward only comes at terminal states - we would like the following consistency to hold:\n",
    "\n",
    "$\\hat{r}(i;\\boldsymbol{w}) \\approx \\sum_{k} s_{ik} \\bar{z}_k + \\gamma \\sum_{j} Q_{ij} \\hat{r}(j;\\boldsymbol{w})$\n",
    "\n",
    "Assuming we have a good state representation set $X$, we want to learn $\\boldsymbol{w}$ from experience. \n",
    "\n",
    "Say we take a step from state $i$ to state $j$ - both non-terminal. For these states, we can move towards this approximation by, on each step, minimising the temporal error:\n",
    "\n",
    "$\\epsilon = | \\hat{r}(i;\\boldsymbol{w}) - \\gamma\\hat{r}(j;\\boldsymbol{w}) |^2 = (\\boldsymbol{x}_i^\\intercal\\boldsymbol{w} - \\gamma\\hat{r}(j;\\boldsymbol{w}))^2$\n",
    "\n",
    "e.g. by doing S(semi-)GD:\n",
    "\n",
    "$\\nabla_{\\boldsymbol{w}}\\epsilon = 2 (\\boldsymbol{x}_i^\\intercal\\boldsymbol{w} - \\gamma\\hat{r}(j;\\boldsymbol{w})) \\boldsymbol{x}_i$\n",
    "\n",
    "with some learning rate $\\alpha$:\n",
    "\n",
    "$\\boldsymbol{w} \\longleftarrow \\boldsymbol{w} - \\alpha (\\boldsymbol{x}_i^\\intercal\\boldsymbol{w} - \\gamma\\boldsymbol{x}_j^\\intercal\\boldsymbol{w}) \\boldsymbol{x}_i$\n",
    "\n",
    "and if it was a terminal state, where we received an immediate reward of $z$, then:\n",
    "\n",
    "$\\epsilon = (\\boldsymbol{x}_i^\\intercal\\boldsymbol{w} - z)^2 \\therefore \\boldsymbol{w} \\longleftarrow \\boldsymbol{w} - \\alpha (\\boldsymbol{x}_i^\\intercal\\boldsymbol{w} - z) \\boldsymbol{x}_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But what should the representation set $X$ look like?\n",
    "\n",
    "We want the linear approximation $\\boldsymbol{r}$ to include information about the successors of the current state. Therefore, \"a good representation for a state would be one that resembles the representations of its successors\"\n",
    "\n",
    "For example, we can define $X_{ij} = [\\boldsymbol{x}_{i}]_j$ to be the *discounted expected occupancy of state j after starting at state i*\n",
    "\n",
    "That is: $X_{ij} = \\sum_{\\tau=0}^\\infty \\gamma^\\tau p(s_{t + \\tau} = j | s_t = i)$\n",
    "\n",
    "So: $X = I + \\gamma Q + \\gamma^2 Q^2 + ... = [I - \\gamma Q]^{-1}$\n",
    "\n",
    "Then, we immediately have the optimal weight matrix for this: $\\boldsymbol{w}^* = \\boldsymbol{h}$\n",
    "\n",
    "Here, $\\boldsymbol{x}_i$ is called the *successor representation* of state $i$.\n",
    "\n",
    "Sneakily, we have combined the terminal and non-terminal states, and have combined $Q$ and $s$ - but the key idea still stands \n",
    "\n",
    "## TD learning of successor representation\n",
    "\n",
    "We have a similar consistency which we can approximate using gradient updates:\n",
    "\n",
    "$X \\approx I + \\gamma QX$\n",
    "\n",
    "When we observe a step from $i$ to $j$, we can update:\n",
    "\n",
    "$\\forall k: \\quad X_{ik} \\longleftarrow X_{ik} - \\alpha [X_{ik} - (\\delta_{jk} + \\gamma X_{jk})]$ \n",
    "\n",
    "This update follows intuitively from the original definition of $X_{ij}$ - we increment $X_{ik}$ towards a $\\delta_{ik} + \\gamma X_{jk}$, for all $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint TD learning\n",
    "\n",
    "We can learn $X$ and $\\boldsymbol{w}$ jointly - using the updates defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State values to action values\n",
    "\n",
    "We now consider the setting where in each state $i$ we have access to a fixed, discrete set of actions $a\\in\\mathcal{A}$\n",
    "\n",
    "$\\pi(a|i)$ = the probability of choosing action $a$ in state $i$\n",
    "\n",
    "$\\hat r_\\pi(j; \\boldsymbol{w})$ = linear state value estimator assuming we follow policy $\\pi$\n",
    "\n",
    "$h_i(a)$ = expected termination reward from state $i$, conditional on taking action $a$\n",
    "\n",
    "We are faced with the challenge of learning *action values*, a.k.a. *Q-values* $\\hat{q}_\\pi(i;\\boldsymbol{w}) \\approx h_i(a) + \\gamma \\sum_j Q_{ij}(a) \\hat r_\\pi(j; \\boldsymbol{w})$\n",
    "\n",
    "It ideally abides by the similar consistency: $\\hat{q}_\\pi(i;\\boldsymbol{w}) \\approx \\sum_{a} \\pi(a | i) \\left[ \\sum_{k} s_{ik}(a) \\bar{z}_k + \\gamma \\sum_{j} Q_{ij}(a) \\hat{q}_\\pi(j;\\boldsymbol{w}) \\right]$\n",
    "\n",
    "The equivalent successor representation is now the tensor: $X^\\pi_{iaj} = \\sum_{\\tau=0}^\\infty \\gamma^\\tau \\sum_{j\\in\\mathcal{S}} p(s_{t + \\tau} = j | s_t = i, a_t = a, a_{>t}\\sim\\pi)$\n",
    "\n",
    "It's definition is now the \"*discounted expected occupancy of state j after starting at state i and taking action a, then following policy $\\pi$ thereafter*\"\n",
    "\n",
    "TD learning of both the SR and the return approximation looks the same, except rules also involve indexing over the action selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key benefit of learning SR\n",
    "\n",
    "Given a well-learned successor representation, we effectively have a model of the world and its transition probabilities, independent of the reward structure (sort of - reward structure will impact learned policy, which in turn impacts the action-conditional SR)\n",
    "\n",
    "So, if the reward structure changes, the agent can retain its state representations (or bootstrap learning from pretrained SR), and just (or mainly) learn the q-value linear approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "class GridWorld:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        height: int,\n",
    "        width: int,\n",
    "        reward_cells: List[Tuple[int]],\n",
    "        prob_correct = 0.5,\n",
    "        term_reward = 1.0,\n",
    "    ):\n",
    "\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "        self.prob_correct = prob_correct\n",
    "        self.term_reward = term_reward\n",
    "\n",
    "        self.current_state = None\n",
    "\n",
    "        self.reward_cells = set(reward_cells)\n",
    "        for i, j in reward_cells:\n",
    "            assert 0<=i<self.height\n",
    "            assert 0<=j<self.width\n",
    "\n",
    "    def init(self) -> None:\n",
    "        #while (self.current_state in self.reward_cells) or (self.current_state is None):\n",
    "        #    self.current_state = (np.random.randint(self.height), np.random.randint(self.width))\n",
    "        self.current_state = (\n",
    "            np.random.choice([0, self.height - 1]), \n",
    "            np.random.choice([0, self.width - 1]),\n",
    "        )\n",
    "\n",
    "    def det_step_inner(self, action: int) -> Tuple[int, int]:\n",
    "        new_state = [self.current_state[0], self.current_state[1]]\n",
    "        if action == 0: # N\n",
    "            new_state[0] = min(new_state[0] + 1, self.height - 1)\n",
    "        if action == 1: # E\n",
    "            new_state[1] = min(new_state[1] + 1, self.width - 1)\n",
    "        if action == 2: # W\n",
    "            new_state[1] = max(new_state[1] - 1, 0)\n",
    "        if action == 3: # S\n",
    "            new_state[0] = max(new_state[0] - 1, 0)\n",
    "        return tuple(new_state)\n",
    "\n",
    "    def step(self, action: int) -> Tuple[Tuple[int], Tuple[int], float, bool]:\n",
    "        \"\"\"\n",
    "        Deterministic step self.prob_correct of the time\n",
    "        0, 1, 2, 3 = N E W S\n",
    "        \"\"\"\n",
    "        u = np.random.random([])\n",
    "        if u <= self.prob_correct:\n",
    "            new_state = self.det_step_inner(action)\n",
    "        else:\n",
    "            new_state = self.det_step_inner(np.random.randint(4))\n",
    "        old_state = tuple(self.current_state)\n",
    "        if new_state in self.reward_cells:\n",
    "            reward = self.term_reward\n",
    "            terminal_flag = True\n",
    "            self.init()\n",
    "        else:\n",
    "            reward = 0.0\n",
    "            terminal_flag = False\n",
    "            self.current_state = new_state\n",
    "        return old_state, new_state, reward, terminal_flag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "from matplotlib.pyplot import Axes\n",
    "\n",
    "class GridAgent:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        height: int,\n",
    "        width: int,\n",
    "        discount_factor: float,\n",
    "        learning_rate: float\n",
    "    ):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "        assert 0.0 <= discount_factor <= 1.0\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        assert learning_rate > 0\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    @abstractmethod\n",
    "    def take_action(self, state: Tuple[int]) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def observe_transition(\n",
    "        self, \n",
    "        old_state: Tuple[int],\n",
    "        new_state: Tuple[int],\n",
    "        action: int,\n",
    "        transition_reward: float,\n",
    "        terminal: bool\n",
    "    ) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def visualise(self, *axes: Axes):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "\n",
    "class EpsilonGreedyQAgent(GridAgent):\n",
    "    \n",
    "    def __init__(self, epsilon: float, height: int, width: int, discount_factor: float, learning_rate: float):\n",
    "        super().__init__(height, width, discount_factor, learning_rate)\n",
    "        self.epsilon = epsilon\n",
    "        self.q_values = np.zeros([height, width, 4])\n",
    "\n",
    "    def get_q_values(self, state: Tuple[int]) -> int:\n",
    "        i, j = state\n",
    "        return self.q_values[i,j]\n",
    "\n",
    "    def change_q_value(self, state: Tuple[int], action: int, change: float) -> None:\n",
    "        i, j = state\n",
    "        self.q_values[i,j,action] += self.learning_rate * change\n",
    "\n",
    "    def take_action(self, state: Tuple[int]) -> int:\n",
    "        u = np.random.random([])\n",
    "        if u <= self.epsilon:\n",
    "            return np.random.randint(4)\n",
    "        else:\n",
    "            return np.argmax(self.get_q_values(state))\n",
    "\n",
    "    def observe_transition(self, old_state: Tuple[int], new_state: Tuple[int], action: int, transition_reward: float, terminal: bool) -> None:\n",
    "        bootstrapped_new_value = (\n",
    "            transition_reward if terminal else\n",
    "            transition_reward + self.discount_factor * self.get_q_values(new_state).max()\n",
    "        )\n",
    "        td_error = bootstrapped_new_value - self.get_q_values(old_state)[action]\n",
    "        self.change_q_value(old_state, action, td_error)\n",
    "    \n",
    "    def visualise(self, axes: List[Axes]):\n",
    "        assert len(axes) == 4\n",
    "        for i, ax in enumerate(axes):\n",
    "            ax.imshow(self.q_values[:,:,i], origin = 'lower')\n",
    "            ax.set_title('NEWS'[i])\n",
    "\n",
    "        \n",
    "class SuccessorRepresentationEpsilonGreedyQAgent(GridAgent):\n",
    "    \n",
    "    def __init__(self, epsilon: float, height: int, width: int, discount_factor: float, learning_rate: float, learning_rate_v: float):\n",
    "        super().__init__(height, width, discount_factor, learning_rate)\n",
    "        self.epsilon = epsilon\n",
    "        self.sf_M = np.zeros([height, width, 4, height, width])    # (s, a) -> s'\n",
    "        self.w = np.random.randn(height, width)\n",
    "        self.learning_rate_v = learning_rate_v\n",
    "\n",
    "    def take_action(self, state: Tuple[int]) -> int:\n",
    "        u = np.random.random([])\n",
    "        if u <= self.epsilon:\n",
    "            return np.random.randint(4)\n",
    "        else:\n",
    "            return np.argmax(self.get_q_values(state))\n",
    "\n",
    "    def get_q_values(self, state: Tuple[int]) -> int:\n",
    "        i, j = state\n",
    "        from_state_sf_M = self.sf_M[i,j]    # [4, h, w]\n",
    "        return (from_state_sf_M * self.w[None]).sum(-1).sum(-1)   # [4]\n",
    "\n",
    "    def change_q_value(self, state: Tuple[int], action: int, change: float) -> None:\n",
    "        i, j = state\n",
    "        self.w += self.learning_rate_v * change * self.sf_M[i,j,action]\n",
    "        \n",
    "    def observe_transition(self, old_state: Tuple[int], new_state: Tuple[int], action: int, transition_reward: float, terminal: bool) -> None:\n",
    "\n",
    "        # Update M[s, a, s'] for s\n",
    "        bootstrapped_new_sr = np.zeros([self.height, self.width]) # I[new state, j] + \\gamma * M[new state, j] for all j\n",
    "        bootstrapped_new_sr[new_state[0], new_state[1]] = 1.0\n",
    "        if not terminal:\n",
    "            bootstrapped_new_sr = bootstrapped_new_sr + self.discount_factor * self.sf_M[:, :, action, new_state[0], new_state[1]]\n",
    "        td_error = bootstrapped_new_sr - self.sf_M[:, :, action, old_state[0], old_state[1]]\n",
    "        self.sf_M[old_state[0], old_state[1], action] += self.learning_rate * td_error\n",
    "\n",
    "        # Update Q[s, a] for s, a, by changing w\n",
    "        bootstrapped_new_value = (\n",
    "            transition_reward if terminal else\n",
    "            transition_reward + self.discount_factor * self.get_q_values(new_state).max()\n",
    "        )\n",
    "        td_error = bootstrapped_new_value - self.get_q_values(old_state)[action]\n",
    "        self.change_q_value(old_state, action, td_error)\n",
    "\n",
    "    def visualise(self, axes: List[Axes], source_state: Optional[Tuple[int]]):\n",
    "        assert len(axes) >= 5\n",
    "        for i, ax in enumerate(axes[:4]):\n",
    "            ax.imshow(self.sf_M[source_state[0], source_state[1], i], origin = 'lower')\n",
    "            ax.set_title('NEWS'[i] + f' from {source_state}')\n",
    "        axes[-1].set_title('w')\n",
    "        axes[-1].imshow(self.w, origin = 'lower')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:03<00:00, 78.90it/s] \n",
      " 79%|███████▊  | 3932/5000 [01:18<00:21, 49.98it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m agent\u001b[39m.\u001b[39mvisualise(axes[\u001b[39m1\u001b[39m])\n\u001b[1;32m     63\u001b[0m fig\u001b[39m.\u001b[39msuptitle(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mns\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mworld\u001b[39m.\u001b[39mreward_cells\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m fig\u001b[39m.\u001b[39;49msavefig(\u001b[39m'\u001b[39;49m\u001b[39mq_values\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     66\u001b[0m latest_episode \u001b[39m=\u001b[39m [world\u001b[39m.\u001b[39mcurrent_state]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/container/lib/python3.8/site-packages/matplotlib/figure.py:2311\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2308\u001b[0m         patch\u001b[39m.\u001b[39mset_facecolor(\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   2309\u001b[0m         patch\u001b[39m.\u001b[39mset_edgecolor(\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 2311\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mprint_figure(fname, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2313\u001b[0m \u001b[39mif\u001b[39;00m transparent:\n\u001b[1;32m   2314\u001b[0m     \u001b[39mfor\u001b[39;00m ax, cc \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes, original_axes_colors):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/container/lib/python3.8/site-packages/matplotlib/backend_bases.py:2210\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     _bbox_inches_restore \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2210\u001b[0m     result \u001b[39m=\u001b[39m print_method(\n\u001b[1;32m   2211\u001b[0m         filename,\n\u001b[1;32m   2212\u001b[0m         dpi\u001b[39m=\u001b[39;49mdpi,\n\u001b[1;32m   2213\u001b[0m         facecolor\u001b[39m=\u001b[39;49mfacecolor,\n\u001b[1;32m   2214\u001b[0m         edgecolor\u001b[39m=\u001b[39;49medgecolor,\n\u001b[1;32m   2215\u001b[0m         orientation\u001b[39m=\u001b[39;49morientation,\n\u001b[1;32m   2216\u001b[0m         bbox_inches_restore\u001b[39m=\u001b[39;49m_bbox_inches_restore,\n\u001b[1;32m   2217\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2218\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   2219\u001b[0m     \u001b[39mif\u001b[39;00m bbox_inches \u001b[39mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/container/lib/python3.8/site-packages/matplotlib/backend_bases.py:1639\u001b[0m, in \u001b[0;36m_check_savefig_extra_args.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1631\u001b[0m     cbook\u001b[39m.\u001b[39mwarn_deprecated(\n\u001b[1;32m   1632\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m3.3\u001b[39m\u001b[39m'\u001b[39m, name\u001b[39m=\u001b[39mname,\n\u001b[1;32m   1633\u001b[0m         message\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m() got unexpected keyword argument \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1634\u001b[0m                 \u001b[39m+\u001b[39m arg \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m which is no longer supported as of \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1635\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m and will become an error \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1636\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1637\u001b[0m     kwargs\u001b[39m.\u001b[39mpop(arg)\n\u001b[0;32m-> 1639\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/container/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:509\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39m@_check_savefig_extra_args\u001b[39m\n\u001b[1;32m    461\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_png\u001b[39m(\u001b[39mself\u001b[39m, filename_or_obj, \u001b[39m*\u001b[39margs,\n\u001b[1;32m    462\u001b[0m               metadata\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, pil_kwargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    463\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[39m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m     FigureCanvasAgg\u001b[39m.\u001b[39;49mdraw(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    510\u001b[0m     mpl\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mimsave(\n\u001b[1;32m    511\u001b[0m         filename_or_obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_rgba(), \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpng\u001b[39m\u001b[39m\"\u001b[39m, origin\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mupper\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    512\u001b[0m         dpi\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure\u001b[39m.\u001b[39mdpi, metadata\u001b[39m=\u001b[39mmetadata, pil_kwargs\u001b[39m=\u001b[39mpil_kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/container/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:407\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[39m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[39mwith\u001b[39;00m RendererAgg\u001b[39m.\u001b[39mlock, \\\n\u001b[1;32m    405\u001b[0m      (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoolbar\u001b[39m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoolbar\n\u001b[1;32m    406\u001b[0m       \u001b[39melse\u001b[39;00m nullcontext()):\n\u001b[0;32m--> 407\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdraw(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrenderer)\n\u001b[1;32m    408\u001b[0m     \u001b[39m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     \u001b[39m# don't forget to call the superclass.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mdraw()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/container/lib/python3.8/site-packages/matplotlib/artist.py:41\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     42\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/container/lib/python3.8/site-packages/matplotlib/figure.py:1863\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1860\u001b[0m             \u001b[39m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 1863\u001b[0m     mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[1;32m   1864\u001b[0m         renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[1;32m   1866\u001b[0m     renderer\u001b[39m.\u001b[39mclose_group(\u001b[39m'\u001b[39m\u001b[39mfigure\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1867\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/container/lib/python3.8/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/container/lib/python3.8/site-packages/matplotlib/artist.py:41\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     42\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/container/lib/python3.8/site-packages/matplotlib/cbook/deprecation.py:411\u001b[0m, in \u001b[0;36m_delete_parameter.<locals>.wrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m     deprecation_addendum \u001b[39m=\u001b[39m (\n\u001b[1;32m    402\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIf any parameter follows \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m!r}\u001b[39;00m\u001b[39m, they should be passed as \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mkeyword, not positionally.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    404\u001b[0m     warn_deprecated(\n\u001b[1;32m    405\u001b[0m         since,\n\u001b[1;32m    406\u001b[0m         name\u001b[39m=\u001b[39m\u001b[39mrepr\u001b[39m(name),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    409\u001b[0m                  \u001b[39melse\u001b[39;00m deprecation_addendum,\n\u001b[1;32m    410\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 411\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49minner_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minner_kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/container/lib/python3.8/site-packages/matplotlib/axes/_base.py:2747\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2744\u001b[0m         a\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[1;32m   2745\u001b[0m     renderer\u001b[39m.\u001b[39mstop_rasterizing()\n\u001b[0;32m-> 2747\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(renderer, \u001b[39mself\u001b[39;49m, artists)\n\u001b[1;32m   2749\u001b[0m renderer\u001b[39m.\u001b[39mclose_group(\u001b[39m'\u001b[39m\u001b[39maxes\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   2750\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstale \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/container/lib/python3.8/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/container/lib/python3.8/site-packages/matplotlib/artist.py:41\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     42\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/container/lib/python3.8/site-packages/matplotlib/image.py:643\u001b[0m, in \u001b[0;36m_ImageBase.draw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         renderer\u001b[39m.\u001b[39mdraw_image(gc, l, b, im, trans)\n\u001b[1;32m    642\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 643\u001b[0m     im, l, b, trans \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_image(\n\u001b[1;32m    644\u001b[0m         renderer, renderer\u001b[39m.\u001b[39;49mget_image_magnification())\n\u001b[1;32m    645\u001b[0m     \u001b[39mif\u001b[39;00m im \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    646\u001b[0m         renderer\u001b[39m.\u001b[39mdraw_image(gc, l, b, im)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/container/lib/python3.8/site-packages/matplotlib/image.py:928\u001b[0m, in \u001b[0;36mAxesImage.make_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    925\u001b[0m transformed_bbox \u001b[39m=\u001b[39m TransformedBbox(bbox, trans)\n\u001b[1;32m    926\u001b[0m clip \u001b[39m=\u001b[39m ((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_clip_box() \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39mbbox) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_clip_on()\n\u001b[1;32m    927\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure\u001b[39m.\u001b[39mbbox)\n\u001b[0;32m--> 928\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_image(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_A, bbox, transformed_bbox, clip,\n\u001b[1;32m    929\u001b[0m                         magnification, unsampled\u001b[39m=\u001b[39;49munsampled)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/container/lib/python3.8/site-packages/matplotlib/image.py:561\u001b[0m, in \u001b[0;36m_ImageBase._make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    556\u001b[0m     output[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m3\u001b[39m] \u001b[39m=\u001b[39m output_alpha  \u001b[39m# recombine rgb and alpha\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[39m# at this point output is either a 2D array of normed data\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[39m# (of int or float)\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[39m# or an RGBA array of re-sampled input\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto_rgba(output, \u001b[39mbytes\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, norm\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    562\u001b[0m \u001b[39m# output is now a correctly sized RGBA array of uint8\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \n\u001b[1;32m    564\u001b[0m \u001b[39m# Apply alpha *after* if the input was greyscale without a mask\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mif\u001b[39;00m A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/container/lib/python3.8/site-packages/matplotlib/cm.py:333\u001b[0m, in \u001b[0;36mScalarMappable.to_rgba\u001b[0;34m(self, x, alpha, bytes, norm)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[39mif\u001b[39;00m norm:\n\u001b[1;32m    332\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n\u001b[0;32m--> 333\u001b[0m rgba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcmap(x, alpha\u001b[39m=\u001b[39;49malpha, \u001b[39mbytes\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mbytes\u001b[39;49m)\n\u001b[1;32m    334\u001b[0m \u001b[39mreturn\u001b[39;00m rgba\n",
      "File \u001b[0;32m/opt/anaconda3/envs/container/lib/python3.8/site-packages/matplotlib/colors.py:610\u001b[0m, in \u001b[0;36mColormap.__call__\u001b[0;34m(self, X, alpha, bytes)\u001b[0m\n\u001b[1;32m    606\u001b[0m         lut[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m alpha\n\u001b[1;32m    607\u001b[0m         \u001b[39m# If the bad value is set to have a color, then we\u001b[39;00m\n\u001b[1;32m    608\u001b[0m         \u001b[39m# override its alpha just as for any other value.\u001b[39;00m\n\u001b[0;32m--> 610\u001b[0m rgba \u001b[39m=\u001b[39m lut[xa]\n\u001b[1;32m    611\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39miterable(X):\n\u001b[1;32m    612\u001b[0m     \u001b[39m# Return a tuple if the input was a scalar\u001b[39;00m\n\u001b[1;32m    613\u001b[0m     rgba \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(rgba)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "height = 5\n",
    "width = 5\n",
    "\n",
    "\n",
    "world = GridWorld(\n",
    "    height = height,\n",
    "    width = width,\n",
    "    reward_cells = [(2, 2)],\n",
    "    prob_correct = 0.5,\n",
    "    term_reward = 10.0\n",
    ")\n",
    "\n",
    "agent = EpsilonGreedyQAgent(\n",
    "    epsilon = 1.0,\n",
    "    height = height,\n",
    "    width = width,\n",
    "    discount_factor = 0.99,\n",
    "    learning_rate = 0.01,\n",
    ")\n",
    "\n",
    "world.init()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize = (12, 8))\n",
    "\n",
    "num_steps = 5_000\n",
    "\n",
    "for ns in tqdm(range(num_steps)):\n",
    "\n",
    "    action = agent.take_action(world.current_state)\n",
    "\n",
    "    old_state, new_state, reward, terminal_flag = world.step(action)\n",
    "\n",
    "    agent.observe_transition(old_state, new_state, action, reward, terminal_flag)\n",
    "\n",
    "    if terminal_flag:\n",
    "        plt.close('all')\n",
    "\n",
    "        agent.visualise(axes[0])\n",
    "        fig.suptitle(f\"{ns}\\n{world.reward_cells}\")\n",
    "        fig.savefig('q_values')\n",
    "\n",
    "\n",
    "world.reward_cells = {(2, 0)}\n",
    "\n",
    "for ns in tqdm(range(num_steps)):\n",
    "\n",
    "    action = agent.take_action(world.current_state)\n",
    "\n",
    "    old_state, new_state, reward, terminal_flag = world.step(action)\n",
    "\n",
    "    agent.observe_transition(old_state, new_state, action, reward, terminal_flag)\n",
    "\n",
    "    latest_episode.append(new_state)\n",
    "\n",
    "    if terminal_flag:\n",
    "        plt.close('all')\n",
    "\n",
    "        agent.visualise(axes[1])\n",
    "        fig.suptitle(f\"{ns}\\n{world.reward_cells}\")\n",
    "        fig.savefig('q_values')\n",
    "\n",
    "        latest_episode = [world.current_state]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 136.27it/s]\n",
      "100%|██████████| 1000/1000 [00:07<00:00, 135.50it/s]\n"
     ]
    }
   ],
   "source": [
    "height = 5\n",
    "width = 5\n",
    "\n",
    "\n",
    "world = GridWorld(\n",
    "    height = height,\n",
    "    width = width,\n",
    "    reward_cells = [(2, 2)],\n",
    "    prob_correct = 0.7,\n",
    "    term_reward = 10.0\n",
    ")\n",
    "\n",
    "agent = SuccessorRepresentationEpsilonGreedyQAgent(\n",
    "    epsilon = 1.0,\n",
    "    height = height,\n",
    "    width = width,\n",
    "    discount_factor = 0.99,\n",
    "    learning_rate = 0.01,\n",
    "    learning_rate_v = 1.0,\n",
    ")\n",
    "\n",
    "world.init()\n",
    "\n",
    "num_steps = 5_000\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize = (15, 8))\n",
    "\n",
    "\n",
    "for ns in tqdm(range(num_steps)):\n",
    "\n",
    "    action = agent.take_action(world.current_state)\n",
    "\n",
    "    old_state, new_state, reward, terminal_flag = world.step(action)\n",
    "\n",
    "    agent.observe_transition(old_state, new_state, action, reward, terminal_flag)\n",
    "\n",
    "    if terminal_flag:\n",
    "        plt.close('all')\n",
    "\n",
    "        agent.visualise(axes[0], (2, 1))\n",
    "        fig.suptitle(f\"{ns}\\n{world.reward_cells}\")\n",
    "        fig.savefig('sr_q_values')\n",
    "\n",
    "world.reward_cells = {(2, 0)}\n",
    "\n",
    "for ns in tqdm(range(num_steps)):\n",
    "\n",
    "    action = agent.take_action(world.current_state)\n",
    "\n",
    "    old_state, new_state, reward, terminal_flag = world.step(action)\n",
    "\n",
    "    agent.observe_transition(old_state, new_state, action, reward, terminal_flag)\n",
    "\n",
    "    if terminal_flag:\n",
    "        plt.close('all')\n",
    "\n",
    "        agent.visualise(axes[1], (2, 1))\n",
    "        fig.suptitle(f\"{ns}\\n{world.reward_cells}\")\n",
    "        fig.savefig('sr_q_values')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.20 ('container')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "690068dc79f78991e5f0946fa17434e3277aa5e164c730a43da8be33cce1155b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
